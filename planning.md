# Research Plan: Recursive Language Models for Cross-Modal Inference

## 1. Research Question
Does a **Recursive Visual In-Context Learning (R-VICL)** approach, which recursively chunks and synthesizes patterns from visual examples, outperform standard "all-at-once" In-Context Learning (ICL) in Multimodal Large Language Models (MLLMs) on long-context tasks?

## 2. Background and Motivation
Standard MLLMs struggle with long contexts containing many visual examples (Many-Shot ICL). They often lose track of patterns or hallucinate when the context window is crowded.
**Recursive Language Models (RLMs)** propose breaking down inputs into chunks, processing them hierarchically (recursively), and summarizing the information.
We hypothesize that applying this recursive summarization/pattern-extraction to *visual* examples will improve performance on visual reasoning tasks compared to standard flat prompting.

## 3. Hypothesis Decomposition
- **H1:** Recursive chunking of in-context examples allows the model to extract more robust "rules" than linear processing.
- **H2:** R-VICL will show greater improvement as the number of shots (examples) increases (scaling law of recursion).

## 4. Methodology

### Dataset
- **MMLongBench**: Specifically the **Many-Shot ICL** subset.
- **Data Location**: `datasets/MMLongBench/mmlb_data` (Text/Meta) and `datasets/MMLongBench/3_icl_image.tar.gz` (Images).

### Models
1.  **Baseline (Standard ICL)**:
    -   Model: GPT-4o (via API).
    -   Method: Feed all $N$ examples + Query Image into the context window directly.
2.  **Proposed (Recursive ICL)**:
    -   Model: GPT-4o (via API).
    -   Method:
        1.  **Chunking**: Divide $N$ examples into chunks of size $k$ (e.g., $k=2$).
        2.  **Leaf Recursion**: For each chunk, ask the LLM: "Analyze these examples. What is the common rule or pattern mapping the input image to the output text?"
        3.  **Node Recursion**: Summarize the rules from the chunks into a "Master Rule".
        4.  **Inference**: Apply the "Master Rule" to the Query Image.

### Evaluation
- **Metric**: Accuracy (Exact Match / F1 as defined in MMLongBench).
- **Protocol**:
    -   Run on a subset of MMLongBench ICL tasks (due to API costs/time).
    -   Compare Baseline vs. Proposed on the *same* examples.

## 5. Implementation Plan

### Phase 2: Setup
1.  Create `uv` environment.
2.  Install `MMLongBench` dependencies.
3.  Unpack Image Data (`3_icl_image.tar.gz`).
4.  Verify data loading.

### Phase 3: Implementation
1.  **`src/recursive_model.py`**: Implement the `RecursiveICLModel` class.
    -   Inherits from standard `OpenAIModel` wrapper.
    -   Overrides `generate` to implement the chunking logic.
2.  **`src/run_eval.py`**: A simplified runner script based on `code/MMLongBench/eval.py`.

### Phase 4: Experiments
1.  **Baseline Run**: Run GPT-4o on ICL task (standard).
2.  **Recursive Run**: Run `RecursiveICLModel` on ICL task.
3.  **Ablation (Optional)**: Vary chunk size.

### Phase 5: Analysis
- Calculate accuracy.
- Analyze "Rules" generated by the recursive model (qualitative analysis).
- Error analysis.

## 6. Timeline
- **Setup**: 15 min
- **Implementation**: 45 min
- **Experiments**: 45 min (Parallelized if possible)
- **Analysis**: 20 min
- **Documentation**: 20 min

## 7. Success Criteria
- The Recursive model achieves higher accuracy than the Baseline.
- The "Master Rule" is human-interpretable and correct.

## 8. Potential Challenges
- **API Costs**: Many-shot ICL consumes many tokens. I will limit the test set size (e.g., 20-50 samples) to ensure feasibility.
- **Latency**: Recursive calls take longer. This is acceptable for research but worth noting.
- **Image Handling**: Ensuring images are passed correctly to the chunks.